{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_attention_network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDFVR4r3wtTL"
      },
      "source": [
        "Note: If running on Google Colab, make sure to create a data/ folder and src/ folder in the runtime with all of the necessary sub-folders and files. Something like this:\r\n",
        "\r\n",
        "![Screenshot 2021-01-01 181113.png](data:image/png;base64,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)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75vfJ6rSoGIl"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.utils.data import Dataset\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import time\r\n",
        "import math\r\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqz7RFMGpMtL"
      },
      "source": [
        "class EnVietDataset(Dataset):\r\n",
        "    def __init__(self, en_path, viet_path, en_vocab_path, viet_vocab_path):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        en_inputs = []\r\n",
        "        viet_translations = []\r\n",
        "\r\n",
        "        with open(en_path, 'r', encoding='utf-8') as en_f:\r\n",
        "            for en_line in en_f.readlines():\r\n",
        "                en_sequence = en_line.strip()\r\n",
        "                en_tokens = en_sequence.split(' ')\r\n",
        "                en_tokens.insert(0, '<s>')\r\n",
        "                en_tokens.append('</s>')\r\n",
        "                en_inputs.append(en_tokens)\r\n",
        "\r\n",
        "        with open(viet_path, 'r', encoding='utf-8') as viet_f:\r\n",
        "            for viet_line in viet_f.readlines():\r\n",
        "                viet_sequence = viet_line.strip()\r\n",
        "                viet_tokens = viet_sequence.split(' ')\r\n",
        "                viet_tokens.insert(0, '<s>')\r\n",
        "                viet_tokens.append('</s>')\r\n",
        "                viet_translations.append(viet_tokens)\r\n",
        "\r\n",
        "        # Vocab maps english tokens to indices then reverse vocab maps indices to english tokens\r\n",
        "        en_vocab = self._build_vocab(en_vocab_path)\r\n",
        "        en_reverse_vocab = {index: token for token, index in en_vocab.items()}\r\n",
        "\r\n",
        "        # Vocab maps vietnamese tokens to indices then reverse vocab maps indices to vietnamese tokens\r\n",
        "        viet_vocab = self._build_vocab(viet_vocab_path)\r\n",
        "        viet_reverse_vocab = {index: token for token, index in viet_vocab.items()}\r\n",
        "\r\n",
        "        self.en_vocab = en_vocab\r\n",
        "        self.en_reverse_vocab = en_reverse_vocab\r\n",
        "\r\n",
        "        self.viet_vocab = viet_vocab\r\n",
        "        self.viet_reverse_vocab = viet_reverse_vocab\r\n",
        "\r\n",
        "        indexed_en_inputs = [self.tokens_to_indices(en_input, lang='en') for en_input in en_inputs]\r\n",
        "        indexed_viet_translations = [self.tokens_to_indices(viet_translation, lang='viet') for viet_translation in viet_translations]\r\n",
        "\r\n",
        "        self.en_inputs = indexed_en_inputs\r\n",
        "        self.viet_translations = indexed_viet_translations\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        return self.en_inputs[index], self.viet_translations[index]\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.en_inputs)\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def _build_vocab(vocab_path):\r\n",
        "        \"\"\"Builds a vocab (dictionary) of word->index.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            vocab_path (str): Path to the vocab.\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            dict of word->index: The vocab of word->index.\r\n",
        "        \"\"\"\r\n",
        "        assert os.path.exists(vocab_path)\r\n",
        "\r\n",
        "        vocab = {'<pad>': 0}\r\n",
        "        token_id = 1\r\n",
        "\r\n",
        "        with open(vocab_path, 'r', encoding='utf-8') as f:\r\n",
        "            for line in f.readlines():\r\n",
        "                token = line.strip()\r\n",
        "                vocab[token] = token_id\r\n",
        "                token_id += 1\r\n",
        "\r\n",
        "        return vocab\r\n",
        "\r\n",
        "    def tokens_to_indices(self, tokens, lang='en'):\r\n",
        "        \"\"\"Converts a list of tokens from strings to their corresponding indices in the specified vocab.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            tokens (list of str's): Tokens to be converted.\r\n",
        "            lang (str, optional): Specifies which vocab to use. Defaults to 'en' for English. Other option\r\n",
        "            is 'viet' for Vietnamese.\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            length-N tensor: Tensor containing the indices corresponding to each token.\r\n",
        "        \"\"\"\r\n",
        "        assert lang == 'en' or lang == 'viet'\r\n",
        "\r\n",
        "        indices = []\r\n",
        "        vocab = self.en_vocab if lang == 'en' else self.viet_vocab\r\n",
        "\r\n",
        "        unk_token = vocab['<unk>']\r\n",
        "\r\n",
        "        for token in tokens:\r\n",
        "            indices.append(vocab.get(token, unk_token))\r\n",
        "\r\n",
        "        return torch.tensor(indices)\r\n",
        "\r\n",
        "    def indices_to_tokens(self, indices, lang='en'):\r\n",
        "        \"\"\"Converts indices to tokens and concatenates them as a string.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            indices (list of str's): A tensor of indices (with shape (N, 1) or length-N), a list of (1, 1) tensors,\r\n",
        "            or a list of indices (ints).\r\n",
        "            lang (str, optional): Specifies which vocab to use. Defaults to 'en' for English. Other option\r\n",
        "            is 'viet' for Vietnamese.\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            str: String from concatenating the tokens.\r\n",
        "        \"\"\"\r\n",
        "        assert lang == 'en' or lang == 'viet'\r\n",
        "\r\n",
        "        tokens = []\r\n",
        "        reverse_vocab = self.en_reverse_vocab if lang == 'en' else self.viet_reverse_vocab\r\n",
        "\r\n",
        "        for index in indices:\r\n",
        "            if torch.is_tensor(index):\r\n",
        "                index = index.item()\r\n",
        "            token = reverse_vocab.get(index, '<unk>')\r\n",
        "            if token == '<pad>':\r\n",
        "                continue\r\n",
        "            tokens.append(token)\r\n",
        "\r\n",
        "        return \" \".join(tokens)\r\n",
        "\r\n",
        "def collate_fn(batch):\r\n",
        "    \"\"\"Create a batch of data given a list of N input sequences and output sequences. Returns a tuple\r\n",
        "    containing two tensors each with shape (N, max_sequence_length), where max_sequence_length is the\r\n",
        "    maximum length of any sequence in the batch.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        batch (list): A list of size N, where each element is a tuple containing two sequence tensors.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        tuple of two tensors, list of ints, list of ints: A tuple containing two tensors each with\r\n",
        "    shape (N, max_sequence_length), list of each input sequence's length, and list of each target\r\n",
        "    sequence's length.\r\n",
        "    \"\"\"\r\n",
        "    en_inputs, viet_translations = zip(*batch)\r\n",
        "    max_en_input_length = 0\r\n",
        "    max_viet_translation_length = 0\r\n",
        "\r\n",
        "    e = []\r\n",
        "    v = []\r\n",
        "    e_lens = []\r\n",
        "    v_lens = []\r\n",
        "\r\n",
        "    for en_input in en_inputs:\r\n",
        "        en_input_length = list(en_input.size())[0]\r\n",
        "        e_lens.append(en_input_length)\r\n",
        "        if en_input_length > max_en_input_length:\r\n",
        "            max_en_input_length = en_input_length\r\n",
        "    for en_input in en_inputs:\r\n",
        "        en_input_length = list(en_input.size())[0]\r\n",
        "        if en_input_length < max_en_input_length:\r\n",
        "            e.append(torch.cat((en_input, torch.zeros(max_en_input_length - en_input_length, dtype=int))))\r\n",
        "        else:\r\n",
        "            e.append(en_input)\r\n",
        "\r\n",
        "    for viet_translation in viet_translations:\r\n",
        "        viet_translation_length = list(viet_translation.size())[0]\r\n",
        "        v_lens.append(viet_translation_length)\r\n",
        "        if viet_translation_length > max_viet_translation_length:\r\n",
        "            max_viet_translation_length = viet_translation_length\r\n",
        "    for viet_translation in viet_translations:\r\n",
        "        viet_translation_length = list(viet_translation.size())[0]\r\n",
        "        if viet_translation_length < max_viet_translation_length:\r\n",
        "            v.append(torch.cat((viet_translation, torch.zeros(max_viet_translation_length - viet_translation_length, dtype=int))))\r\n",
        "        else:\r\n",
        "            v.append(viet_translation)\r\n",
        "\r\n",
        "    return (torch.stack(e), torch.stack(v)), e_lens, v_lens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6krnKbZpNvC"
      },
      "source": [
        "class AttnEncoderRNN(nn.Module):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\r\n",
        "        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\r\n",
        "\r\n",
        "    def forward(self, batch_sequences, seq_lens):\r\n",
        "        \"\"\"Forward pass through the encoder.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            batch_sequences (N-by-seq_len tensor): Batch containing N length-seq_len tensors\r\n",
        "        (e.g., the sequences to be translated). N is the batch size.\r\n",
        "            seq_lens (list of ints): List of sequences lengths of each batch element.\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            tuple of one (N, seq_len, hidden_size) tensor and two (1, N, hidden_size) tensors: All hidden states of each\r\n",
        "            sequence in the batch and (hn, cn) from the RNN (LSTM) layer.\r\n",
        "        \"\"\"\r\n",
        "        batch_sequences = self.embedding(batch_sequences)    # N-by-seq_len-by-embedding_dim\r\n",
        "\r\n",
        "        packed_batch_sequences = nn.utils.rnn.pack_padded_sequence(batch_sequences, lengths=seq_lens, batch_first=True, enforce_sorted=False)\r\n",
        "\r\n",
        "        out, (hn, cn) = self.rnn(packed_batch_sequences)    # hn and cn are both 1-by-N-by-hidden_size\r\n",
        "\r\n",
        "        # Unpack output from RNN (LSTM) layer. out_padded is N-by-seq_len-by-hidden_size\r\n",
        "        out_padded, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\r\n",
        "\r\n",
        "        # out_padded: all hidden states of each sequence in the batch\r\n",
        "        # hn: the final hidden state of each sequence in the batch\r\n",
        "        # cn: final cell state of each sequence in the batch\r\n",
        "        return out_padded, hn, cn\r\n",
        "\r\n",
        "class AttnDecoderRNN(nn.Module):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\r\n",
        "        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\r\n",
        "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\r\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "    def forward(self, prev_outputs, prev_hn, prev_cn, encoder_hidden_states, device):\r\n",
        "        \"\"\"Forward pass through the decoder.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            prev_outputs (N-by-1): The ouputs from the previous time step. N is the batch size.\r\n",
        "            prev_hn (1-by-N-by-hidden_size tensor):\r\n",
        "            prev_cn (1-by-N-by-hidden_size tensor):\r\n",
        "            encoder_hidden_states (N-input_seq_len-by-hidden_size tensor):\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            tuple of one (N, vocab_size) tensor and two (1, N, hidden_size) tensors: The predicted outputs and (hn, cn) from\r\n",
        "        the RNN (LSTM) layer.\r\n",
        "        \"\"\"\r\n",
        "        embeddings = self.embedding(prev_outputs)    # N-by-1-by-embedding_dim\r\n",
        "        out, (hn, cn) = self.rnn(embeddings, (prev_hn, prev_cn))    # out is N-by-1-by-hidden_size, hn and cn are both 1-by-N-by-hidden_size\r\n",
        "\r\n",
        "        alignment_scores = torch.sum(encoder_hidden_states * out, dim=2, keepdim=True)    # N-by-input_seq_len-by-1\r\n",
        "\r\n",
        "        context_vectors = torch.sum(encoder_hidden_states * alignment_scores, dim=1)    # N-by-hidden_size\r\n",
        "\r\n",
        "        # torch.sum(out, dim=1) is a neat way to squeeze the dimensions to N-by-hidden_size, since\r\n",
        "        # torch.squeeze(out) won't with a batch size N = 1.\r\n",
        "        concat = torch.cat((torch.sum(out, dim=1), context_vectors), dim=1)    # N-by-hidden_size*2\r\n",
        "        concat = self.softmax(self.fc(concat))    # N-by-vocab_size\r\n",
        "\r\n",
        "        return concat, hn, cn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P64Fu-mupYlY"
      },
      "source": [
        "def train(input_batch, target_batch, e_lens, v_lens, encoder, decoder, encoder_optim, decoder_optim, loss_fn, device):\r\n",
        "    all_encoder_hidden_states, all_encoder_hn, all_encoder_cn = encoder(input_batch, e_lens)    # (N, seq_len, hidden_size), (1, N, hidden_size), (1, N, hidden_size)\r\n",
        "\r\n",
        "    decoder_inputs = target_batch[:,0:1]    # N-by-1; the <s> from each sequence\r\n",
        "    prev_hn = all_encoder_hn\r\n",
        "    prev_cn = all_encoder_cn\r\n",
        "\r\n",
        "    preds = []\r\n",
        "    targets = []\r\n",
        "\r\n",
        "    max_seq_len = max(v_lens)\r\n",
        "    for time_step in range(max_seq_len - 1):\r\n",
        "        outputs, hn, cn = decoder(decoder_inputs, prev_hn, prev_cn, all_encoder_hidden_states, device)\r\n",
        "\r\n",
        "        preds.append(outputs)\r\n",
        "        targets.append(target_batch[:,time_step+1])\r\n",
        "\r\n",
        "        top_pred_vals, indices = outputs.topk(1)    # N-by-1 and N-by-1\r\n",
        "        decoder_inputs = indices.detach()\r\n",
        "        prev_hn = hn\r\n",
        "        prev_cn = cn\r\n",
        "\r\n",
        "    loss = loss_fn(torch.cat(preds, dim=0), torch.cat(targets, dim=0))\r\n",
        "\r\n",
        "    encoder_optim.zero_grad()\r\n",
        "    decoder_optim.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    encoder_optim.step()\r\n",
        "    decoder_optim.step()\r\n",
        "\r\n",
        "    return loss.item()\r\n",
        "\r\n",
        "def evaluate(input_seq, input_seq_len, encoder, decoder, cutoff=300):\r\n",
        "    encoder_hidden_states, encoder_hn, encoder_cn = encoder(input_seq, input_seq_len)\r\n",
        "\r\n",
        "    decoder_input = torch.tensor(2)\r\n",
        "    prev_hn = encoder_hn\r\n",
        "    prev_cn = encoder_cn\r\n",
        "\r\n",
        "    predicted_indices = []\r\n",
        "\r\n",
        "    # Model could potentially keep generating words on forever; use a cutoff limit to restrict this\r\n",
        "    for i in range(cutoff):\r\n",
        "        output, hn, cn = decoder(torch.tensor([[decoder_input.item()]]), prev_hn, prev_cn, encoder_hidden_states, device)\r\n",
        "        prev_hn = hn\r\n",
        "        prev_cn = cn\r\n",
        "        top_pred_val, top_pred_idx = output.topk(1)    # largest output and its corresponding index\r\n",
        "        decoder_input = torch.tensor(top_pred_idx.item())\r\n",
        "        predicted_indices.append(decoder_input.item())\r\n",
        "        if decoder_input.item() == 3: break    # predicted '</s>', so stop\r\n",
        "\r\n",
        "    return torch.tensor(predicted_indices)\r\n",
        "\r\n",
        "def asMinutes(s):\r\n",
        "    m = math.floor(s / 60)\r\n",
        "    s -= m * 60\r\n",
        "    return '%dm %ds' % (m, s)\r\n",
        "\r\n",
        "def timeSince(since):\r\n",
        "    now = time.time()\r\n",
        "    s = now - since\r\n",
        "    return asMinutes(s)\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    # Check GPU availability\r\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "    print(device)\r\n",
        "    print()\r\n",
        "\r\n",
        "    total_train_set_size = 133300\r\n",
        "    batch_size = 100\r\n",
        "\r\n",
        "    # Change as needed\r\n",
        "    learning_rate = 0.005\r\n",
        "    momentum = 0.9\r\n",
        "    embedding_dim = 256\r\n",
        "    hidden_size = 512\r\n",
        "    num_epochs = 1\r\n",
        "\r\n",
        "    en_path = './data/train/train.en.txt'\r\n",
        "    viet_path = './data/train/train.vi.txt'\r\n",
        "    en_vocab_path = './data/vocab/vocab.en.txt'\r\n",
        "    viet_vocab_path = './data/vocab/vocab.vi.txt'\r\n",
        "    train_dataset = EnVietDataset(en_path, viet_path, en_vocab_path, viet_vocab_path)\r\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\r\n",
        "\r\n",
        "    # Print out some random examples from the data\r\n",
        "    print(\"Data examples:\")\r\n",
        "    random_indices = torch.randperm(len(train_dataset))[:8].tolist()\r\n",
        "    for index in random_indices:\r\n",
        "        en_indices, viet_indices = train_dataset.en_inputs[index], train_dataset.viet_translations[index]\r\n",
        "        en_input = train_dataset.indices_to_tokens(en_indices, lang='en')\r\n",
        "        viet_translation = train_dataset.indices_to_tokens(viet_indices, lang='viet')\r\n",
        "        print(f\"English: {en_input}. Vietnamese: {viet_translation}\")\r\n",
        "    print()\r\n",
        "\r\n",
        "    encoder = AttnEncoderRNN(len(train_dataset.en_vocab), embedding_dim, hidden_size)\r\n",
        "    decoder = AttnDecoderRNN(len(train_dataset.viet_vocab), embedding_dim, hidden_size)\r\n",
        "\r\n",
        "    encoder.to(device)\r\n",
        "    decoder.to(device)\r\n",
        "\r\n",
        "    encoder_optim = optim.SGD(encoder.parameters(), lr=learning_rate, momentum=momentum)\r\n",
        "    decoder_optim = optim.SGD(decoder.parameters(), lr=learning_rate, momentum=momentum)\r\n",
        "\r\n",
        "    loss_fn = nn.NLLLoss()\r\n",
        "\r\n",
        "    training_losses = []\r\n",
        "\r\n",
        "    start = time.time()\r\n",
        "\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        encoder.train()\r\n",
        "        decoder.train()\r\n",
        "        for i, data in enumerate(train_loader):\r\n",
        "            # Training on Colab's GPU throws an ambiguous error probably involving the last batch not having batch_size since\r\n",
        "            # it's all the remaining sentence pairs (which is less than batch_size). So, don't train on the last batch.\r\n",
        "            if i == (total_train_set_size // batch_size): break\r\n",
        "\r\n",
        "            en, viet, e_lens, v_lens = data[0][0].to(device), data[0][1].to(device), data[1], data[2]\r\n",
        "            batch_loss = train(en, viet, e_lens, v_lens, encoder, decoder, encoder_optim, decoder_optim, loss_fn, device)\r\n",
        "\r\n",
        "            training_losses.append(batch_loss)\r\n",
        "\r\n",
        "            # Print every 20 mini-batches\r\n",
        "            if i % 20 == 19:\r\n",
        "                print(f'[Epoch {epoch + 1}, Batch {i + 1} ({(i + 1) * batch_size} translations)] ({timeSince(start)}): {batch_loss}')\r\n",
        "\r\n",
        "    plt.figure(1)\r\n",
        "    plt.title('NLL Loss per Batch')\r\n",
        "    plt.xlabel(f'Batch (1 batch = {batch_size} translations)')\r\n",
        "    plt.ylabel('NLL Loss')\r\n",
        "    plt.plot(training_losses)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    torch.save(encoder.state_dict(), './src/trained_models/attention_encoder.pth')\r\n",
        "    torch.save(decoder.state_dict(), './src/trained_models/attention_decoder.pth')\r\n",
        "\r\n",
        "    test_enc = AttnEncoderRNN(len(train_dataset.en_vocab), embedding_dim, hidden_size)\r\n",
        "    test_enc.load_state_dict(torch.load('./src/trained_models/attention_encoder.pth'))\r\n",
        "    test_dec = AttnDecoderRNN(len(train_dataset.viet_vocab), embedding_dim, hidden_size)\r\n",
        "    test_dec.load_state_dict(torch.load('./src/trained_models/attention_decoder.pth'))\r\n",
        "\r\n",
        "    test_enc.eval()\r\n",
        "    test_dec.eval()\r\n",
        "    while True:\r\n",
        "        en_input = input('> English: ')\r\n",
        "        if en_input == '<STOP>': break\r\n",
        "        en_input_tokens = en_input.strip().split(' ')\r\n",
        "        en_input_tokens.insert(0, '<s>')\r\n",
        "        en_input_tokens.append('</s>')\r\n",
        "        en_input_indices = train_dataset.tokens_to_indices(en_input_tokens, lang='en')\r\n",
        "        test_en_input = torch.zeros((1, len(en_input_tokens)))\r\n",
        "        test_en_input[0] = en_input_indices\r\n",
        "        test_en_input = test_en_input.long()\r\n",
        "        test_en_input = test_en_input\r\n",
        "        with torch.no_grad():\r\n",
        "            predicted_indices = evaluate(test_en_input, [len(en_input_tokens)], test_enc, test_dec)\r\n",
        "            print(f'> Vietnamese: {train_dataset.indices_to_tokens(predicted_indices, lang=\"viet\")}')\r\n",
        "            print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}